{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ad77ab-4d23-44a3-bee2-ea6495941688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hollymandel/miniconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/hollymandel/miniconda3/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: (__ZN3c1017RegisterOperatorsD1Ev)\n",
      "  Referenced from: '/Users/hollymandel/miniconda3/lib/python3.10/site-packages/torchvision/image.so'\n",
      "  Expected in: '/Users/hollymandel/miniconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import transformer as transformer\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "SEQ_LENGTH = 20\n",
    "PRINT_EVERY = 10\n",
    "N_EPOCHS = 10\n",
    "LR = 1e-3\n",
    "ES_PATIENCE = 10\n",
    "BATCH_SIZE = 2**7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195bb6ce-cf6f-4055-bbb2-ef8a40dada00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"shakespeare_indices.pkl\", 'rb') as file:\n",
    "    ind = np.asarray(pickle.load(file))\n",
    "with open(\"shakespeare_token_dict.pkl\", 'rb') as file:\n",
    "    token_dict = pickle.load(file)  \n",
    "vocab_size = len(token_dict.keys()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e3788d-2a5a-486d-99ef-c4cad42b6852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class shakes_data(Dataset):\n",
    "    def __init__(self, ind, N = SEQ_LENGTH):\n",
    "        overhang = ind.shape[0] % N\n",
    "        resid_len = ind.shape[0] - overhang\n",
    "        data = ind[overhang:]\n",
    "        data = data.reshape(int(resid_len/N),N)\n",
    "        self.data = data[:,:-1]\n",
    "        self.labels = data[:,1:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "    \n",
    "sh = shakes_data(ind, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297f448e-983b-4a0c-9f9b-2b5f048ff02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hotify(vec: np.ndarray, vocab_size):\n",
    "    oh = np.zeros([len(vec), vocab_size])\n",
    "    for i in range(vec.size):\n",
    "        oh[i, vec[i]] = 1\n",
    "    return oh\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=ES_PATIENCE, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.infty\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.best_model = None\n",
    "\n",
    "    def __call__(self, loss, model):\n",
    "        if loss > self.best_loss + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.counter = 0\n",
    "            if loss < self.best_loss:\n",
    "                self.best_model = copy.deepcopy(model)\n",
    "                self.best_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4249a4ac-ca3c-4ff4-90cf-5187b5703f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade notebook ipython jupyterlab\n",
    "# %matplotlib notebook\n",
    "import time\n",
    "import copy\n",
    "\n",
    "def format_input(data):\n",
    "    return torch.from_numpy(\n",
    "        np.apply_along_axis(\n",
    "            one_hotify, \n",
    "            axis=1, \n",
    "            arr=data, \n",
    "            vocab_size = vocab_size\n",
    "        ).astype(np.float32))\n",
    "\n",
    "def this_word(inputs):\n",
    "    return [ index_dict[x] for x in inputs[0,:] ]\n",
    "\n",
    "def next_word(model, inputs):\n",
    "    model_probs = model(format_input(inputs))\n",
    "    max_prob = np.argmax(model_probs.detach().numpy(),2)\n",
    "    return [ index_dict[x] for x in max_prob[0,:] ]\n",
    "\n",
    "def get_grad_sum(model):\n",
    "    all_grad_norms = [ np.linalg.norm(param.grad.detach().numpy()) for param in model.parameters() ]\n",
    "    # plt.plot(all_grad_norms)\n",
    "    return np.sum(all_grad_norms) \n",
    "\n",
    "def test_loss(model, testloader, criterion):\n",
    "    total_loss = 0.0\n",
    "    total_grad = 0.0\n",
    "    for i, dt in enumerate(testloader, 0):\n",
    "            inputs, labels = dt\n",
    "            inputs = format_input(inputs.detach().numpy())\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.permute(0, 2, 1), labels)\n",
    "            total_loss += loss.item()\n",
    "            total_grad += get_grad_sum(model)\n",
    "    return total_loss, total_grad\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    trainloader,\n",
    "    testloader,\n",
    "    save_out = None,\n",
    "    optimizer_type = torch.optim.Adam, \n",
    "    criterion = nn.CrossEntropyLoss(), \n",
    "    lr = LR,\n",
    "    n_epochs = N_EPOCHS, \n",
    "    print_every = PRINT_EVERY,\n",
    "    es_patience = ES_PATIENCE,\n",
    "    verbose=True\n",
    "):\n",
    "    save_out = save_out or []\n",
    "    optimizer = optimizer_type(model.parameters(), lr)\n",
    "    es = EarlyStopping(patience=es_patience, verbose=False)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, dt in enumerate(trainloader, 0):\n",
    "            inputs, labels = dt\n",
    "            inputs = format_input(inputs.detach().numpy())\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.permute(0, 2, 1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % print_every == print_every-1:\n",
    "                if verbose:\n",
    "                    model.eval()\n",
    "                    this_train, this_train_grad = running_loss / print_every, get_grad_sum(model)\n",
    "                    this_test, this_test_grad = test_loss(model, testloader, criterion)\n",
    "                    save_out.append([this_train, this_train_grad, this_test, this_test_grad])\n",
    "                    print(f\"train gradients: {this_train_grad:.0f}, test gradients: {this_test_grad:.0f}\")\n",
    "                    print(f\"[epoch {epoch+1},batch {i+1}] train loss: {this_train:.2f}, test loss: {this_test:.2f}\")\n",
    "                es(running_loss, model)\n",
    "                if es.early_stop:\n",
    "                    return es.best_model, save_out\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    return es.best_model, save_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "360c7840-2420-4a47-852a-2957205c3111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train gradients: 5, test gradients: 427\n",
      "[epoch 1,batch 10] train loss: 10.68, test loss: 731.87\n",
      "train gradients: 4, test gradients: 326\n",
      "[epoch 1,batch 20] train loss: 8.83, test loss: 638.74\n",
      "train gradients: 3, test gradients: 253\n",
      "[epoch 1,batch 30] train loss: 7.79, test loss: 573.44\n",
      "train gradients: 3, test gradients: 204\n",
      "[epoch 1,batch 40] train loss: 7.19, test loss: 544.64\n",
      "train gradients: 2, test gradients: 191\n",
      "[epoch 1,batch 50] train loss: 6.92, test loss: 533.94\n",
      "train gradients: 2, test gradients: 187\n",
      "[epoch 1,batch 60] train loss: 6.80, test loss: 528.32\n",
      "train gradients: 2, test gradients: 172\n",
      "[epoch 1,batch 70] train loss: 6.74, test loss: 524.78\n",
      "train gradients: 2, test gradients: 169\n",
      "[epoch 1,batch 80] train loss: 6.70, test loss: 522.11\n",
      "train gradients: 2, test gradients: 168\n",
      "[epoch 1,batch 90] train loss: 6.64, test loss: 519.97\n",
      "train gradients: 2, test gradients: 155\n",
      "[epoch 1,batch 100] train loss: 6.67, test loss: 518.01\n",
      "train gradients: 2, test gradients: 165\n",
      "[epoch 1,batch 110] train loss: 6.67, test loss: 516.31\n"
     ]
    }
   ],
   "source": [
    "train_fraction = 0.8\n",
    "train_size = int(train_fraction * len(sh))\n",
    "test_size = len(sh) - train_size\n",
    "trainset, testset = random_split(sh, [train_size, test_size])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "save_out = []\n",
    "model = transformer.Transformer(n_blocks=2, n_heads = 8, N = SEQ_LENGTH-1, M = 50, embed_dim = vocab_size, d_target = vocab_size,)\n",
    "model, save_out = train_model(\n",
    "    model = model,\n",
    "    trainloader = trainloader,\n",
    "    testloader = testloader,\n",
    "    save_out = save_out,\n",
    "    lr = 1e-3,\n",
    "    verbose=True,\n",
    "    print_every = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59be45e-f34d-4e60-88ee-c056ff570166",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def compute_loss(model, data_loader, criterion):\n",
    "#     loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in data_loader:\n",
    "#             images = torch.from_numpy(\n",
    "#                 np.apply_along_axis(\n",
    "#                     one_hotify, \n",
    "#                     axis=1, \n",
    "#                     arr=images.detach().numpy(), \n",
    "#                     vocab_size = vocab_size\n",
    "#                 ).astype(np.float32))\n",
    "#             outputs = model(images)\n",
    "#             loss += criterion(outputs.permute(0, 2, 1), labels.to(torch.long))\n",
    "#     return loss\n",
    "\n",
    "# def grid_search(learning_rates = [1e-3, 5e-4, 1e-4], n_heads = [4,8,16], n_blocks = [2,4,8], batch_sizes = [64,256,1024], models_dict = None, save_file = None, save_every = 1):\n",
    "#     models_dict = models_dict or {}\n",
    "#     shared_tf_params = {\"N\": SEQ_LENGTH-1, \"M\": 50, \"embed_dim\": vocab_size, \"d_target\": vocab_size}\n",
    "#     losses = np.zeros([len(batch_sizes), len(n_heads), len(learning_rates), len(n_blocks)])\n",
    "#     save_ix = 0\n",
    "#     for i in range(len(batch_sizes)):\n",
    "#         bs = batch_sizes[i]\n",
    "#         data_loader = DataLoader(sh, batch_size=bs, shuffle=True)\n",
    "#         for j in range(len(n_heads)):\n",
    "#             for k in range(len(learning_rates)):\n",
    "#                 for l in range(len(n_blocks)):\n",
    "#                     nh, lr, nb = n_heads[j], learning_rates[k], n_blocks[l]\n",
    "#                     this_model = transformer.Transformer(n_heads=nh, n_blocks=l, **shared_tf_params)\n",
    "#                     train_model(this_model , data_loader, lr = lr)\n",
    "#                     losses[i,j,k,l] = compute_loss(this_model, data_loader, nn.CrossEntropyLoss())\n",
    "#                     models_dict[(bs, nh, lr, nb)] = this_model\n",
    "#                     print(f\"completed batch size: {bs}, n_heads: {nh}, learning_rate: {lr}\")\n",
    "#                     if save_file and save_ix % save_every == save_every-1:\n",
    "#                         with open(f\"{save_file}.pkl\", 'wb') as f:\n",
    "#                             pickle.dump(losses, f)\n",
    "#                             print(f\"saved losses at model index {save_ix}\")\n",
    "#                     save_ix += 1\n",
    "                \n",
    "#     return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c048c44-72c6-4c9e-af16-8c6931d2a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(data):\n",
    "    return torch.from_numpy(\n",
    "        np.apply_along_axis(\n",
    "            one_hotify, \n",
    "            axis=1, \n",
    "            arr=data, \n",
    "            vocab_size = vocab_size\n",
    "        ).astype(np.float32))\n",
    "\n",
    "def this_word(inputs):\n",
    "    return [ index_dict[x] for x in inputs[0,:] ]\n",
    "\n",
    "def next_word(model, inputs):\n",
    "    model_probs = model(format_input(inputs))\n",
    "    max_prob = np.argmax(model_probs.detach().numpy(),2)\n",
    "    return [ index_dict[x] for x in max_prob[0,:] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b8da3f9-0ffa-4cef-b77f-0bc86c61d92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'the',\n",
       " 'i',\n",
       " 'and',\n",
       " 'to',\n",
       " 'and',\n",
       " 'of',\n",
       " 'i',\n",
       " 'the',\n",
       " 'lord',\n",
       " 'of',\n",
       " 'a',\n",
       " 'have',\n",
       " 'and',\n",
       " 'i',\n",
       " 'that',\n",
       " 'lord',\n",
       " 'the',\n",
       " 'in']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample = np.random.randint(len(sh))\n",
    "\n",
    "next_word(model, sh[random_sample][0][None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84abe0b7-977a-4cd0-8fe3-c0bba1f6fc78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
