{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e0a38a-7db5-4164-9cb5-2b7b2b46d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install --upgrade swig\n",
    "# !pip install --upgrade box2D\n",
    "# !pip3 install box2d box2d-kengz\n",
    "# !pip install pygame\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09aa66-dcb4-4254-ae97-32da893a6778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hollymandel/Documents/llm_curriculum/6-rl/ppo.py:105: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  V_all = V(torch.Tensor(obs_all))[:,0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] reward: -1.93  clip (pct): 0.0 delta S (bps): 1.5  S: 344.3 advantage mean (bps): -46205.7 advantage sd: 18.81  V quality (pct): -0.026\n",
      "[1] reward: -1.93  clip (pct): 0.1 delta S (bps): 3.3  S: 343.4 advantage mean (bps): -45969.6 advantage sd: 18.79  V quality (pct): -0.026\n",
      "[2] reward: -1.79  clip (pct): 0.1 delta S (bps): 3.6  S: 342.5 advantage mean (bps): -42280.7 advantage sd: 18.33  V quality (pct): -0.025\n",
      "[3] reward: -1.65  clip (pct): 0.4 delta S (bps): 5.1  S: 341.0 advantage mean (bps): -39449.4 advantage sd: 18.12  V quality (pct): -0.024\n",
      "[4] reward: -1.51  clip (pct): 0.5 delta S (bps): 6.7  S: 338.7 advantage mean (bps): -35564.5 advantage sd: 17.58  V quality (pct): -0.025\n",
      "[5] reward: -1.51  clip (pct): 1.4 delta S (bps): 9.8  S: 334.8 advantage mean (bps): -36126.3 advantage sd: 17.21  V quality (pct): -0.028\n",
      "[6] reward: -1.43  clip (pct): 1.6 delta S (bps): 11.6  S: 330.0 advantage mean (bps): -34511.7 advantage sd: 16.57  V quality (pct): -0.031\n",
      "[7] reward: -1.44  clip (pct): 1.2 delta S (bps): 10.4  S: 324.5 advantage mean (bps): -34084.3 advantage sd: 15.70  V quality (pct): -0.033\n",
      "[8] reward: -1.48  clip (pct): 1.0 delta S (bps): 8.4  S: 320.0 advantage mean (bps): -35175.1 advantage sd: 15.11  V quality (pct): -0.040\n",
      "[9] reward: -1.50  clip (pct): 0.9 delta S (bps): 6.0  S: 316.2 advantage mean (bps): -35412.8 advantage sd: 14.53  V quality (pct): -0.044\n",
      "[10] reward: -1.58  clip (pct): 1.9 delta S (bps): 6.9  S: 316.0 advantage mean (bps): -37844.8 advantage sd: 14.59  V quality (pct): -0.051\n",
      "[11] reward: -1.48  clip (pct): 3.6 delta S (bps): 9.5  S: 317.6 advantage mean (bps): -35496.5 advantage sd: 14.48  V quality (pct): -0.044\n",
      "[12] reward: -1.38  clip (pct): 1.9 delta S (bps): 7.1  S: 316.1 advantage mean (bps): -32671.7 advantage sd: 14.47  V quality (pct): -0.044\n",
      "[13] reward: -1.13  clip (pct): 1.6 delta S (bps): 7.3  S: 314.7 advantage mean (bps): -26797.7 advantage sd: 14.23  V quality (pct): -0.037\n",
      "[14] reward: -1.04  clip (pct): 2.2 delta S (bps): 9.1  S: 311.8 advantage mean (bps): -24479.6 advantage sd: 13.80  V quality (pct): -0.037\n",
      "[15] reward: -1.07  clip (pct): 2.8 delta S (bps): 9.9  S: 308.8 advantage mean (bps): -25223.6 advantage sd: 13.71  V quality (pct): -0.041\n",
      "[16] reward: -1.04  clip (pct): 2.7 delta S (bps): 9.3  S: 306.8 advantage mean (bps): -24486.6 advantage sd: 13.47  V quality (pct): -0.044\n",
      "[17] reward: -0.92  clip (pct): 3.7 delta S (bps): 11.2  S: 305.3 advantage mean (bps): -21919.5 advantage sd: 13.28  V quality (pct): -0.042\n",
      "[18] reward: -0.80  clip (pct): 1.8 delta S (bps): 8.0  S: 303.2 advantage mean (bps): -18905.6 advantage sd: 13.19  V quality (pct): -0.034\n",
      "[19] reward: -0.64  clip (pct): 1.5 delta S (bps): 6.4  S: 301.9 advantage mean (bps): -15088.8 advantage sd: 12.73  V quality (pct): -0.030\n",
      "[20] reward: -0.67  clip (pct): 1.8 delta S (bps): 7.5  S: 297.4 advantage mean (bps): -15906.6 advantage sd: 12.79  V quality (pct): -0.031\n",
      "[21] reward: -0.68  clip (pct): 1.7 delta S (bps): 7.5  S: 291.8 advantage mean (bps): -16220.4 advantage sd: 12.73  V quality (pct): -0.034\n",
      "[22] reward: -0.59  clip (pct): 1.4 delta S (bps): 6.8  S: 290.3 advantage mean (bps): -13926.1 advantage sd: 12.72  V quality (pct): -0.029\n",
      "[23] reward: -0.55  clip (pct): 1.4 delta S (bps): 6.0  S: 284.1 advantage mean (bps): -13147.9 advantage sd: 12.80  V quality (pct): -0.029\n",
      "[24] reward: -0.55  clip (pct): 1.7 delta S (bps): 6.8  S: 278.1 advantage mean (bps): -12943.4 advantage sd: 12.60  V quality (pct): -0.029\n",
      "[25] reward: -0.59  clip (pct): 2.6 delta S (bps): 9.1  S: 271.4 advantage mean (bps): -13805.9 advantage sd: 12.79  V quality (pct): -0.027\n"
     ]
    }
   ],
   "source": [
    "policy = ppo.MLP(8,[20],4)\n",
    "V = ppo.MLP(8,[32,32],1)\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=None)\n",
    "\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
    "V_optimizer = torch.optim.Adam(V.parameters(), lr=1e-5)\n",
    "\n",
    "output_all = []\n",
    "\n",
    "n_epochs = 100\n",
    "for i in range(n_epochs):\n",
    "    output = reward, n_clip, dS, S, advm, advsd, vq = ppo.epoch_train(\n",
    "        env, policy, V, policy_optimizer, V_optimizer, n_samples = int(1e5), inner_batch_size=int(1e3))\n",
    "    output_all.append(output)\n",
    "    \n",
    "    print(\"[%0.0f] reward: %0.2f  clip (pct): %0.1f delta S (bps): %0.1f  S: %0.1f advantage mean (bps): %0.1f advantage sd: %0.2f  V quality (pct): %0.3f\" % ((i,) + output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fad03d-7e52-489b-a1d1-6d2ef2eb63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.plot_ppo_output(output_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4fd66-8531-45df-a6ef-be765ccb82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "while True:\n",
    "    logits = policy.forward(torch.Tensor(obs))\n",
    "    dist = Categorical(logits=logits)\n",
    "    action = dist.sample().item()\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4062ace-1f44-47a4-a244-7b13ebc3f46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936e0e4-6b00-4327-964a-fcb78dbaf905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
